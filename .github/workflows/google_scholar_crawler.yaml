name: Update Google Scholar Citations

on:
  schedule:
    # Runs at 8:00 AM UTC every day
    - cron: '0 8 * * *'
  workflow_dispatch: # Allows manual triggering from the Actions tab
  push:
    branches:
      - main # Or your default branch
    paths:
      - '.github/workflows/google_scholar_crawler.yaml'
      - 'google_scholar_crawler/**'

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    permissions:
      contents: write

    steps:
    - name: Checkout main branch code
      uses: actions/checkout@v4
      with:
        ref: 'main'

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r google_scholar_crawler/requirements.txt

    - name: Run Scraper
      id: scrape
      env:
        GOOGLE_SCHOLAR_ID: ${{ secrets.GOOGLE_SCHOLAR_ID }}
        PYTHONUNBUFFERED: "1"
      run: |
        # The scraper script will now create .json files in the root directory
        python google_scholar_crawler/main.py

    - name: Deploy data to orphan branch
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"

        git checkout --orphan google-scholar-stats
        git rm -rf . || exit 0

        # MODIFIED: Add all .json files from the root directory.
        # Use 'git add -f' to force add them, just in case.
        git add -f ./*.json

        git commit -m "Deploy latest citation data at $(date -u)"
        
        echo "Pushing data-only branch to repository..."
        git push origin google-scholar-stats --force